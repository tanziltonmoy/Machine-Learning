{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Regression is a supervised algorithm to make prediction based on continous y values.  \n",
    "\n",
    "For example, given the following data:\n",
    "\n",
    "| | Egg price  | Gold price    | Oil price   | GDP   |\n",
    "|---:|:-------------|:-----------|:------|:------|\n",
    "| 1 | 3  | 100       | 4   | 21   |\n",
    "| 2 | 4  | 500    | 7   | 43     |\n",
    "\n",
    "We want to use egg price, gold price and oil price to predict GDP.  We called egg price, gold price, oil price **features** or $\\mathbf{X}$. We called what we want to predict **labels** or **targets** or $\\mathbf{y}$.  Each row is called **sample**.  \n",
    "\n",
    "### Course Notations and Terms\n",
    "\n",
    "We shall use the following notations in our course.  \n",
    "\n",
    "- $x_j^{(i)}$ represents the i-th sample, and j-th feature. For example, $x_1^{(1)}$ denote egg price of the first sample (i.e., 100), $x_2^{(1)}$ for gold price of the first sample (i.e., 4), and $x_3^{(1)}$ for oil price of the first sample (i.e., 21).  \n",
    "\n",
    "- Bold captial $\\mathbf{X}$ denotes the whole **matrix** of features with $m$ rows of samples and $n$ columns of features.  The **shape** of $X$ is $(m, n)$.  \n",
    "\n",
    "- Bold lowercase $\\mathbf{x}$ denotes the single **vector** (i.e., column) of feature.\n",
    "\n",
    "- $y^{(i)}$ represents the **targets/labels** of the i-th sample, and $\\mathbf{y}$ refers to the whole **vector** of targets with **shape** of $(m, )$\n",
    "\n",
    "\n",
    "### Hypothesis Function\n",
    "\n",
    "**Hypothesis function** maps given input $\\mathbf{X}$ to predicted $\\mathbf{y}$.  We must **learn**/**train** this function.  To differentiate between actual and predicted $\\mathbf{y}$, we commonly called predicted $\\mathbf{y}$ as $\\hat{\\mathbf{y}}$ (read as yhat).\n",
    "\n",
    "For linear regression, the hypothesis function (denoted as $h_{\\theta}(x)$) which means $h$ depends on $x$ parametized by $\\theta$ is defined as followed.  \n",
    "\n",
    "\\begin{align*}\n",
    "h_\\theta(x) &= \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\cdots + \\theta_nx_n \\tag{A} \\\\\n",
    "&= \\sum_{i=0}^n \\theta_ix_i  \\tag{B} \\\\\n",
    "&= \\boldsymbol{\\theta}^T \\mathbf{x} \\tag{C} \\\\\n",
    "\\mathbf{h} &= \\mathbf{X}\\boldsymbol{\\theta} \\tag{D}\n",
    "\\end{align*}\n",
    "\n",
    "Here $\\theta$ are called **parameters** or **weights** or **coefficients** that parameterize the linear mappings from $\\mathbf{X}$ -> $\\mathbf{y}$.  Also, we commonly don't write equations in the (A) form.  We called (B) form as the **summation** form, (C) as the **vectorized** form, and (D) as the **matrix** form which combines all samples.  We like to write as (D) because it's easy to implement.\n",
    "\n",
    "The resulting hypothesis function is called **model**.  The process is called **training** the model.  The latter process of testing the model on test set is called **inference**.\n",
    "\n",
    "### How to find the best $\\theta$ ==> Gradient Descent\n",
    "\n",
    "How do we learn the best parameters $\\theta$?  \n",
    "\n",
    "First, we must define a **loss/objective function** (denoted as $J$) that measures, for a given $\\theta$, how close $h(\\mathbf{x}^{(i)})$ are to the corresponding $\\mathbf{y}^{(i)}$.  :\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "We want to choose $\\theta$ so as to minimize $J(\\theta)$.  Such process is called **optimization**.\n",
    "\n",
    "$$\\theta^* = \\argmin_\\theta J(\\theta)$$\n",
    "\n",
    "One very successful optimization algorithm is called **gradient descent** algorithm, which is based on updating theta based on the **derivatives** (we also called **gradient**).   **Why gradient descent works?**  \n",
    "\n",
    "This figure illustrates why:\n",
    "\n",
    "<img width=\"400\" src = \"figures/gradient.png\">\n",
    "\n",
    "### Formal Definition\n",
    "\n",
    "We start with some random $\\theta$, and repeatedly performs the update:\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha * \\frac{\\partial J}{\\partial \\theta_j}$$\n",
    "\n",
    " \n",
    "This update is simultaneously performed for all values of $j=0, 1, \\cdots, n$ (if you forget, $j$ here refers to each feature :-).  Here, $\\alpha$ is called the **learning rate (lr)**, ranging from 0 to 1.  Commonly, we tried 0.001 as default, and this value must be manually handpicked.  Any parameters such as learning rate are called **hyperparameters**.\n",
    "\n",
    "In order to implement this algorithm, we have to find the **partial derivative of the loss function in respect to each $\\theta_j$**.  Let's try the partial derivative of our loss function in respect to $\\theta_j$.  Also, let's first work it out for only one training example first as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J}{\\partial \\theta_j} &= \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2}(h_\\theta(x) - y)^2 \\\\\n",
    "&= 2 * \\frac{1}{2} (h_\\theta(x) - y) * \\frac{\\partial}{\\partial \\theta_j} (h_\\theta(x) - y) \\\\\n",
    "&= 2 * \\frac{1}{2} (h_\\theta(x) - y) * \\frac{\\partial}{\\partial \\theta_j} \\big(\\sum_{i=0}^n \\theta_ix_i - y\\big) \\\\\n",
    "&= (h_\\theta(x) - y)x_j \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<!-- This rule has several properties that seem natural and intuitive. For instance, the magnitude of the update is proportional to the **error** term $h_\\theta(x) - y$; thus, for instance, if we are encountering a training example on which our prediction nearly matches the actual value of $y^{(i)}$, then we find that there is little need to change the parameters; in contrast, a larger change to the parameters will be made if our prediction has a large error (i.e., if it is very far from $y^{(i)}$).\n",
    " -->\n",
    " \n",
    "To modify the update rule for whole training example, we revise the update rule to include the summation as\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha * \\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\tag{for every $j$}$$  \n",
    "\n",
    "or\n",
    "\n",
    "$$\\theta = \\theta - \\alpha * \\mathbf{X}^\\top (\\mathbf{h} - \\mathbf{y})$$\n",
    "\n",
    "Since this gradient descent calculates gradient using every example in the entire training set, we called this as **batch gradient descent**.\n",
    "\n",
    "Sometimes, performing batch gradient descent can be slow, thus we can use **stochastic gradient descent** which refers to looking at only one training example, where we can pick with or without replacement.  Here, **without replacement** refers to the process in which no same sample is used in the same **epoch**.  Here epoch means one  iteration which the whole training set is being exhausted.  Thus, in without replacement, we simply loop from $i =1$ to $m$ for one epoch.\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\alpha * ((h_\\theta(x)^{(i)}-y^{(i)})x_j^{(i)}) \\tag{for every $j$}$$\n",
    "\n",
    "Although **stochastic gradient descent** may be faster, it rarely converges to the optimum given its randomness.  A middle ground is **mini-batch gradient descent** which can be expressed as\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha * \\sum_{i=start}^{batchsize}(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\tag{for every $j$}$$\n",
    "\n",
    "Similarly, we can do this with or without replacement.  In without replacement, we simply chop evenly and exhaust the whole training set for one epoch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "The gradient descent has the following steps:\n",
    "\n",
    "1. Prepare your data\n",
    "    - add intercept\n",
    "    - $\\mathbf{X}$ and $\\mathbf{y}$ and $\\mathbf{w}$ in the right shape\n",
    "        - $\\mathbf{X}$ -> $(m, n)$\n",
    "        - $\\mathbf{y}$ -> $(m, )$\n",
    "        - $\\mathbf{w}$ -> $(n, )$\n",
    "    - train-test split\n",
    "    - feature scale\n",
    "    - clean out any missing data\n",
    "    - (optional) feature engineering\n",
    "2. Predict and calculate the loss\n",
    "    - The loss function is the mean squared error\n",
    "    $$J = \\frac{(\\mathbf{h}-\\mathbf{y})^2}{2}$$\n",
    "    where $\\mathbf{h}$ is simply\n",
    "    $$ \\mathbf{h} = X\\boldsymbol{\\theta} $$\n",
    "3. Calculate the gradient based on the loss\n",
    "    - The gradient of the loss function is\n",
    "    $$\\frac{\\partial J}{\\partial \\theta_j} = X^\\top(\\mathbf{h} - \\mathbf{y})$$\n",
    "4. Update the theta with this update rule\n",
    "    $$\\theta = \\theta - \\alpha * \\mathbf{X}^\\top (\\mathbf{h} - \\mathbf{y})$$\n",
    "    where $\\alpha$ is a typical learning rate range between 0 and 1\n",
    "5. Loop 2-4 until `max_iter` is reached, or the difference between old loss and new loss are smaller than some predefined threshold tol"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get our hands dirty!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare data\n",
    "\n",
    "### 1.1 Get your X and y in the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3146058634.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [5], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install --upgrade pip\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 1. Let's load some diabetes data \n",
    "# as our regression case study\n",
    "from sklearn.datasets import load_diabetes\n",
    "# type - Bunch\n",
    "# Bunch - dictionary of numpy data\n",
    "# diabetes.feature_names\n",
    "# print(diabetes)\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "print(\"Features: \", diabetes.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442 10\n"
     ]
    }
   ],
   "source": [
    "X = diabetes.data\n",
    "X.shape #number of samples, number of features\n",
    "\n",
    "m = X.shape[0]  #number of samples\n",
    "n = X.shape[1]  #number of features\n",
    "\n",
    "print(m, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows in X is the same as number of rows in y\n",
    "# because so we have yhat for all y\n",
    "assert m == y.shape[0] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Train test split your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the appropriate size for test data\n",
    "# 70/30 (small dataset); 80/20 (medium dataset); 90/10 (large dataset);\n",
    "# why large dataset, can set test size to 10, because\n",
    "# 10% of large dataset is already enough for testing accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "assert len(X_train)  == len(y_train)\n",
    "assert len(X_test)   == len(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Feature scale your data to reach faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to standardize my data so that mean is 0, variance is 1\n",
    "# average across each feature, NOT across each sample\n",
    "# Why we need to standardize\n",
    "# Because standardizing usually allows us to reach convergence faster\n",
    "# Why -> because the values are within smaller range\n",
    "# Thus, the gradients are also within limited range, and NOT go crazy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. StandardScaler.fit(X)  #this scaler (or self) knows the mean and std so now\n",
    "# it knows how to transform data\n",
    "# 2  X = StandardScaler.transform(X)  #not in place; will return something\n",
    "\n",
    "# 1. StandardScaler.fit_transform(X) -> 1 and 2 sequentially\n",
    "\n",
    "# create an object of StandardScaler\n",
    "# StandardScaler is a class\n",
    "# scaler is called instance/object\n",
    "\n",
    "# ALMOST always, feature scale your data using normalization or standardization\n",
    "# If you assume your data is gaussian, use standardization, otherwise, you do the normalization\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Note: you MUST split first, before scale....if not, you will get data leakage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Add intercepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the shape of X they want\n",
    "# (number of samples, number of features) --> correct shape\n",
    "# How about the intercept\n",
    "# w0 is OUR intercept\n",
    "# what is the shape of w -->(n+1, )\n",
    "# What is the shape of intercept --->(m, 1)\n",
    "#X = [1 2 3     @  [w0\n",
    "#     1 4 6         w1\n",
    "#     1 9 1         w2 \n",
    "#     1 10 2 ] \n",
    "\n",
    "# np.ones((shape))\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "\n",
    "# concatenate the intercept based on axis=1\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "\n",
    "# np.ones((shape))\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "\n",
    "# concatenate the intercept based on axis=1\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Feature Engineering (optional)\n",
    "\n",
    "It is sometimes useful to engineer new features (e.g., polynomial, kernels) so to create some non-linear relationships with your target.\n",
    "\n",
    "Here we gonna skip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fit your algorithm \n",
    "\n",
    "### 2.1 Define your algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "# Step 1: Prepare your data\n",
    "# X_train, X_test have intercepts that are being concatenated to the data\n",
    "# [1, features\n",
    "#  1, features....]\n",
    "\n",
    "# making sure our X_train has same sample size as y_train\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "# initialize our w\n",
    "# We don't have to do X.shape[1] + 1 because our X_train already has the\n",
    "# intercept\n",
    "# w = theta/beta/coefficients\n",
    "theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "# define the learning rate\n",
    "# later on, you gonna know that it should be better to make it slowly decreasing\n",
    "# once we perform a lot of iterations, we want the update to slow down, so it converges better\n",
    "lr = 0.1\n",
    "\n",
    "# define our max_iter\n",
    "# typical to call it epochs <---ml people likes to call it\n",
    "max_iter = 50\n",
    "\n",
    "def h_theta(X, theta):\n",
    "    return X @ theta\n",
    "\n",
    "def mse(yhat, y):\n",
    "    return ((yhat - y)**2).sum() / yhat.shape[0]\n",
    "\n",
    "def gradient(X, error):\n",
    "    m = X.shape[0]\n",
    "    return (X.T @ error) / X.shape[0]\n",
    "\n",
    "start = time()\n",
    "\n",
    "# define your for loop\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    # 1. yhat = X @ w\n",
    "    # prediction\n",
    "    # yhat (m, ) = (m, n) @ (n, )\n",
    "    yhat = h_theta(X_train, theta)\n",
    "\n",
    "    # 2. error = yhat - y_train\n",
    "    # error for use to calculate gradients\n",
    "    # error (m, ) = (m, ) - (m, )\n",
    "    error = yhat - y_train\n",
    "\n",
    "    # 3. grad = X.T @ error\n",
    "    # grad (n, ) = (n, m) @ (m, )\n",
    "    # grad for each feature j\n",
    "    grad = gradient(X_train, error)\n",
    "\n",
    "    # 4. w = w - lr * grad\n",
    "    # update w\n",
    "    # w (n, ) = (n, ) - scalar * (n, )\n",
    "    theta = theta - lr * grad\n",
    "\n",
    "time_taken = time() - start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Compute accuracy/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h_theta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ASUS\\Documents\\GitHub\\Machine-Learning\\01 - Supervised\\01 - Regression\\01 - Gradient Descent.ipynb Cell 21\u001b[0m in \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Documents/GitHub/Machine-Learning/01%20-%20Supervised/01%20-%20Regression/01%20-%20Gradient%20Descent.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# we got our lovely w\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Documents/GitHub/Machine-Learning/01%20-%20Supervised/01%20-%20Regression/01%20-%20Gradient%20Descent.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# now it's time to check our accuracy\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Documents/GitHub/Machine-Learning/01%20-%20Supervised/01%20-%20Regression/01%20-%20Gradient%20Descent.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# 1. Make prediction\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Documents/GitHub/Machine-Learning/01%20-%20Supervised/01%20-%20Regression/01%20-%20Gradient%20Descent.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m yhat \u001b[39m=\u001b[39m h_theta(X_test, theta)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Documents/GitHub/Machine-Learning/01%20-%20Supervised/01%20-%20Regression/01%20-%20Gradient%20Descent.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# 2. Calculate mean squared errors\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Documents/GitHub/Machine-Learning/01%20-%20Supervised/01%20-%20Regression/01%20-%20Gradient%20Descent.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m mse \u001b[39m=\u001b[39m mse(yhat, y_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'h_theta' is not defined"
     ]
    }
   ],
   "source": [
    "# we got our lovely w\n",
    "# now it's time to check our accuracy\n",
    "# 1. Make prediction\n",
    "yhat = h_theta(X_test, theta)\n",
    "\n",
    "# 2. Calculate mean squared errors\n",
    "mse = mse(yhat, y_test)\n",
    "\n",
    "# print the mse\n",
    "print(\"MSE: \", mse)\n",
    "print(\"Time used: \", time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some graphs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(y_test, yhat, c='crimson')\n",
    "\n",
    "p1 = max(max(yhat), max(y_test))\n",
    "p2 = min(min(yhat), min(y_test))\n",
    "plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "plt.xlabel('True Values', fontsize=15)\n",
    "plt.ylabel('Predictions', fontsize=15)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100, 500])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = [ [3, 100, 4], [4, 500, 7]]\n",
    "sample= np.array(list)\n",
    "sample[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3, 100,   4],\n",
       "       [  4, 500,   7]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2. , 0.1, 3. ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.array([2, 0.1,3])\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Workshop - Check your understandings\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "Instruction:  Gather in your group.  Will randomly pick groups to present.\n",
    "\n",
    "1.  Using the table of egg price, gold price, etc., please do the followings:\n",
    "    1.  Draw the vector $\\mathbf{x}_2$\n",
    "    2.  Draw the vector $\\mathbf{x}^{(2)}$\n",
    "    3.  What is $x^{(2)}_1$ = 4\n",
    "    4.  What is $y^{(1)}$ = 21\n",
    "\n",
    "    5.  Draw the vector $\\mathbf{y}$\n",
    "    6.  Draw the entire matrix $\\mathbf{X}$\n",
    "    7.  Create the vector $\\mathbf{x}_2$ using numpy\n",
    "    8.  Create the matrix $\\mathbf{X}$ using numpy\n",
    "2.  Given the first sample $\\mathbf{x}^{(1)}$, and given a weight $\\boldsymbol{\\theta}$ of [2, 0.1, 3], what is $\\hat{y}$? Show the calculation by hand.  (this is called *vectorized* form)\n",
    "3.  Perform 2 using numpy\n",
    "4.  Given the entire matrix $\\mathbf{X}$, and using the same weight as above, what is $\\hat{\\mathbf{y}}$?  Show the calculation by hand.  This is called *matrix* form.\n",
    "5.  Perform 4 using numpy\n",
    "6.  Given the entire matrix $\\mathbf{X}$, given a weight $\\boldsymbol{\\theta}$ of [2, 0.1, 3], and given the $\\mathbf{y}$, what is $J(\\theta)$? Show the calculation by hand.\n",
    "7.  Perform 6 using numpy\n",
    "8.  For training sample 1, i.e., $\\mathbf{x}^{(1)}$, what is $\\displaystyle \\frac{\\partial J}{\\partial \\theta_2}$  Show the calculation by hand.\n",
    "9.  For all samples, i.e., $\\mathbf{X}$, what is $\\displaystyle \\frac{\\partial J}{\\partial \\theta_2}$  Show the calculation by hand.\n",
    "10. For all samples, what is $\\displaystyle \\frac{\\partial J}{\\partial \\theta}$.  Show the calculation by hand.\n",
    "11. Given $\\alpha = 0.01$, what is the new $\\boldsymbol{\\theta}$?\n",
    "12. Prove that $\\theta_j := \\theta_j - \\alpha * \\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$ is same as $\\theta = \\theta - \\alpha * \\mathbf{X}^\\top (\\mathbf{h} - \\mathbf{y})$\n",
    "13. In the beginning of gradient descent, what is the initial weight?\n",
    "14. Review how to find derivatives/gradients, especially multiplication and chain rule by yourself.\n",
    "15. Explain in your own words, why finding the derivatives can help you find the better weight.\n",
    "16. Explain what is stochastic and mini-batch gradient descent.  (actually what you did in 8 is stochastic)\n",
    "17. Now you have transcended.  Go to the code and understand everything :-)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
